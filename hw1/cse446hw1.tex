\documentclass{article}
\linespread{1.3}
\usepackage[margin=50pt]{geometry}
\usepackage{amsmath, amsthm, amssymb, amsthm, tikz, fancyhdr}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\newcommand{\changefont}{\fontsize{15}{15}\selectfont}

\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\E}{\mathbb{E}} 
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\field{R}} % real domain
% \newcommand{\C}{\field{C}} % complex domain
\newcommand{\F}{\field{F}} % functional domain

\newcommand{\T}{^{\textrm T}} % transpose

\def\diag{\text{diag}}

%% operator in linear algebra, functional analysis
\newcommand{\inner}[2]{#1\cdot #2}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\twonorm}[1]{\|#1\|_2^2}
% operator in functios, maps such as M: domain1 --> domain 2
\newcommand{\Map}[1]{\mathcal{#1}}
\renewcommand{\theenumi}{\alph{enumi}} 

\newcommand{\Perp}{\perp \! \! \! \perp}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\vct}[1]{\boldsymbol{#1}} % vector
\newcommand{\mat}[1]{\boldsymbol{#1}} % matrix
\newcommand{\cst}[1]{\mathsf{#1}} % constant
\newcommand{\ProbOpr}[1]{\mathbb{#1}}
\newcommand{\points}[1]{\small\textcolor{magenta}{\emph{[#1 points]}} \normalsize}
\date{{}}

\fancypagestyle{firstpageheader}
{
  \fancyhead[R]{\changefont Michael Huang \\ CSE 446 \\ Homework 1}
}

\begin{document}

\thispagestyle{firstpageheader}

\section*{A.0}
{\Large 

\subsection*{a.}

Bias describes how well a model matches its original training set, while Variance describes how much a model will change when we select different data to train on. \\ \\
The bias-variance tradeoff describes how there is an inverse relationship between bias and variance for a model; that is, we have lower model variance, bias tends to increase, but when we decrease bias, we tend to have higher variance. 

\subsection*{b.}

Typically, with greater model complexity, we have a greater range of data which means we have greater variance, but also lower bias as we typically are fitting on less of the training set. \\
In the same way, with lower model complexity, we have less key data which means there will be a lower variance with fewer options for training data sets, but greater bias as well.

\subsection*{c.}

False. The bias of a model is defined on a single selection of training data, so this doesn't change with more data.

\subsection*{d.}

False. Typically, adding more training data should increase the variance of a model with more options of data to train on.

\subsection*{e.}

False. With less features used to represent the data, this means that we essentially have fewer parameters, which means that we will have more issues generalizing since it is more difficult to tell the trends in the data.

\subsection*{f.}

We should only use the train set to tune hyperparameters. We should only use the test set to test our model, and using it to tune hyperparameters runs a higher risk of overfitting.

\subsection*{g.}

False. The training error of a function on the training set usually is too optimistic when it comes to estimating the true test error, since the test error has greater variance.

}

\section*{A.1}
{\Large

\subsection*{a.}

\begin{verbatim}
  Text enclosed inside \texttt{verbatim}
  environment 
  is printed directly 
  and all \LaTeX{} commands are ignored.
\end{verbatim}

\framebox[1.1\width]{\textbf{answer}}

\subsection*{b.}

\subsection*{c.}

}

\section*{A.2}
{\Large 



}

\section*{A.3}
{\Large 

\subsection*{a.}

\subsection*{b.}

\subsection*{c.}

}

\section*{A.4}
{\Large 



}

\section*{A.5}
{\Large 



}

\section*{A.6}
{\Large 

\subsection*{a.}

\subsection*{b.}

}

\end{document}