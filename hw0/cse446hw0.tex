\documentclass{article}
\linespread{1.3}
\usepackage[margin=50pt]{geometry}
\usepackage{amsmath, amsthm, amssymb, amsthm,fancyhdr, graphicx, tikz}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\newcommand{\changefont}{\fontsize{15}{15}\selectfont}

\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\E}{\mathbb{E}} 
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\field{R}} % real domain
% \newcommand{\C}{\field{C}} % complex domain
\newcommand{\F}{\field{F}} % functional domain

\newcommand{\T}{^{\textrm T}} % transpose

\def\diag{\text{diag}}

%% operator in linear algebra, functional analysis
\newcommand{\inner}[2]{#1\cdot #2}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\twonorm}[1]{\|#1\|_2^2}
% operator in functios, maps such as M: domain1 --> domain 2
\newcommand{\Map}[1]{\mathcal{#1}}
\renewcommand{\theenumi}{\alph{enumi}} 

\newcommand{\Perp}{\perp \! \! \! \perp}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\vct}[1]{\boldsymbol{#1}} % vector
\newcommand{\mat}[1]{\boldsymbol{#1}} % matrix
\newcommand{\cst}[1]{\mathsf{#1}} % constant
\newcommand{\ProbOpr}[1]{\mathbb{#1}}
\newcommand{\points}[1]{\small\textcolor{magenta}{\emph{[#1 points]}} \normalsize}
\date{{}}

\fancypagestyle{firstpageheader}
{
  \fancyhead[R]{\changefont Michael Huang \\ CSE 446 \\ Homework 0}
}

\begin{document}

\thispagestyle{firstpageheader}

\section*{A.1}
{\Large 

Bayes' Rule tells us that $P(A \mid B) = \frac{P(A \cap B)}{P(B)} = \frac{P(B \mid A) \cdot P(A)}{P(B)}$. Let $A$ = the event that we have the disease, and $B$ = the event that we test positive for the disease. We aim to find the probability that we have the disease, given that we tested positive, i.e. $P(A \mid B)$. We are given that $P(A) = \frac{1}{10,000}$, $P(B) = \frac{99}{100}$. \\ \\ 
We need to find $P(B)$, the probability that we test positive for the disease. To do this, we can use the Law of Total Probability to add up the probabilities for its contained disjoint events $A$ and $\lnot A$: \\
$P(B) = P(B | A)P(A) + P(B | \lnot A)P(\lnot A)$ \\ 
$P(B) = \frac{99}{100} \cdot \frac{1}{10,000} + \frac{1}{100} \cdot (1 - \frac{1}{10,000})$ \\
$P(B) = \frac{99}{100} \cdot \frac{1}{10,000} + \frac{1}{100} \cdot \frac{9,999}{10,000}$ \\
$P(B) = \frac{99}{1,000,000} + \frac{9,999}{1,000,000}$ \\
$P(B) = \frac{10,098}{1,000,000}$ \\ \\
Putting this all together, we can find that \\
$P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)} = \frac{\frac{99}{100} \cdot \frac{1}{10,000}}{\frac{10,098}{1,000,000}} = \frac{99}{10,098} = $ \framebox[1.1\width]{\textbf{$\frac{1}{102}$ or $\sim 0.00980392156863$}} \\ 


}

\section*{A.2}
{\Large

\subsection*{a.}
% conditional expectation?
We know that by the definition of covariance: \\
$\text{Cov}(X,Y)=\E[(X-\E[X])(Y-\E[Y])]$ \\
$\text{Cov}(X,Y)=\E[XY - \E[X]Y - \E[Y]X + \E[X]\E[Y]]$ \\
$\text{Cov}(X,Y)=\E[XY] - \E[X]\E[Y] - \E[Y]\E[X] + \E[X]\E[Y]$ \\
$\text{Cov}(X,Y)=\E[XY] - \E[Y]\E[X]$ \\ \\
We are given that $\E[Y \mid X=x ] = x$. This tells us that for any value $X = x$, $E[Y] = x$ as well. We know by definition that $\E[X] = \sum_{x} x \cdot \P(X = x)$. We also know that $\E[Y] = \sum_{x} \E[Y \mid X = x] \cdot \P(X = x)$. \\
Using what we are given, we can easily see that $\E[Y] = \sum_{x} x \cdot \P(X = x) = \E[X]$, or that $\E[X] = \E[Y]$. \\
In the same vein, in our case, using out given information to represent $Y = y$ in terms of $X$, we can determine that $\E[XY] = \sum_x x \cdot y \cdot \P(X = x) = \sum_x x \cdot x \P(X = x) = \E[XX] = \E[X^2]$. In other words, $\E[XY] = \E[X^2]$.\\ \\
Using this information, we can simplify this expression to \\
$\text{Cov}(X,Y)= \E[XY] - \E[X]\E[X]$ \\
$\text{Cov}(X,Y)= \E[X^2] - \E[X]^2$ \\
$\text{Cov}(X,Y)= \text{Var}(X)$ \\
$\text{Cov}(X,Y)= \E[(X - \E[X])^2]$ \\
as desired.

\subsection*{b.}
Since $X, Y$ are independent, then $\E[XY] = \E[X]\E[Y]$. Using the expression that we derived in part a, we can determine that $\text{Cov}(X,Y)=\E[XY] - \E[Y]\E[X] = \E[X]\E[Y] - \E[Y]\E[X] = \E[X]\E[Y] - \E[X]\E[Y] = 0$, as desired.

}

\section*{A.3}
{\Large 

\subsection*{a.}
We aim to find the PDF $h$ of the random variable $Z = X + Y$, where $X$ and $Y$ are independent random variables with PDFs given by $f$ and $g$, respectively. In addition, let $H$, $F$, and $G$ represent the CDFs of $Z$, $X$, and $Y$, respectively. \\ \\
By definition of CDF, we can take \\ 
$H(z) = \P(Z \leq z) = \P(X + Y \leq z)$. \\
$H(z) = \int_{-\infty}^{\infty} \P(X + Y \leq z \mid X = x) f(x) \,dx$ \hfill Law of Total Probability \\ 
$H(z) = \int_{-\infty}^{\infty} \P(x + Y \leq z \mid X = x) f(x) \,dx$ \hfill Definition \\
$H(z) = \int_{-\infty}^{\infty} \P(Y \leq z - x \mid X = x) f(x) \,dx$ \hfill Algebra \\ 
$H(z) = \int_{-\infty}^{\infty} \P(Y \leq z - x) f(x) \,dx$ \hfill Independence of $X$ and $Y$ \\
$H(z) = \int_{-\infty}^{\infty} G(z-x) f(x) \,dx$ \hfill Definition of CDF for $Y$ \\
$h(z) = \frac{d}{dz}\int_{-\infty}^{\infty} G(z-x) f(x) \,dx$ \hfill Definition of PDF \\
$h(z) = \int_{-\infty}^{\infty} g(z-x) f(x) \,dx$ \hfill Definition of PDF \\
as we sought to show.

\subsection*{b.}
We aim to find $h(z)$. Using what we showed in part a, \\
$h(z) = \int_{-\infty}^{\infty} g(z - x) f(x) \,dx$ \\ 
$h(z) = \int_{0}^{1} g(z-x) \,dx$ \hfill Definition of Uniform Distribution \\ \\
We can break this expression into two different cases: where $z = x + y \leq 1$ or not, which we must do since we know that $x$ and $y$ are restricted in the range $[0, 1]$: \\
Case 1: $0 \leq z \leq 1$ \\ 
$h(z) = \int_{0}^{z} g(z-x) \,dx + \int_{z}^{1} g(z-x) \,dx$ \\ 
$h(z) = z \cdot 1 + 0 = z$ \\
Case 2: $1 < z \leq 2$ \\ 
$h(z) = \int_{0}^{z-1} g(z-x) \,dx + \int_{z-1}^{1} g(z-x) \,dx$ \\ 
$h(z) = 1 + 1 - z = 2-z$ \\ \\
Putting this all together, the PDF $h$ is as follows:
\[
h\left(z\right)=\begin{cases}
z & \,0\le z\le1;\\
2 - z & \,1< z\le2;\\
0 & \,\mbox{otherwise,}\\
\end{cases}
\]

}

\section*{A.4}
{\Large 

We aim to find $a, b$ such that $Y = aX + b$. We know that the standard normal is defined by $Y = \frac{X - \mu}{\sigma} = \frac{X}{\sigma} - \frac{\mu}{\sigma} = \frac{1}{\sigma}X - \frac{\mu}{\sigma}$. In the $aX + b$ form, we can see that this translates to \framebox[1.1\width]{\textbf{$a = \frac{1}{\sigma}$ and $b = -\frac{\mu}{\sigma}$}}

}

\section*{A.5}
{\Large 
We aim to find the mean and variance of $Z = \sqrt{n}(\widehat{\mu}_n - \mu)$, where $\widehat{\mu}_n = \frac{1}{n} \sum_{i=1}^n X_i$, and $X_1,\dots,X_n$ are independent and identically distributed random variables, each with mean $\mu$ and variance $\sigma^2$. \\ \\
To find the mean: \\
$\E[Z] = \E[\sqrt{n}(\widehat{\mu}_n - \mu)]$ \\
$\E[Z] = \sqrt{n}\E[\widehat{\mu}_n - \mu]$ \\
$\E[Z] = \sqrt{n}\E[\widehat{\mu}_n] - \E[\mu]$ \\
$\E[Z] = \sqrt{n}(\mu - \mu)$ \hfill Given \\
$\E[Z] = $ \framebox[1.1\width]{\textbf{0}} \\ \\
To find the variance: \\
$\text{Var}(Z) = \E[Z^2] - \E[Z]^2$ \\
$\text{Var}(Z) = \E[Z^2] - 0^2$ \\
$\text{Var}(Z) = \E[Z^2]$ \\
$\text{Var}(Z) = \E[(\sqrt{n}(\widehat{\mu}_n - \mu))^2]$ \\
$\text{Var}(Z) = \E[n \cdot (\widehat{\mu}_n - \mu)^2]$ \\
$\text{Var}(Z) = \E[n \cdot (\widehat{\mu}_n^2 + \mu^2 - 2\widehat{\mu}_n \mu)]$ \\
$\text{Var}(Z) = n \cdot \E[\widehat{\mu}_n^2 + \mu^2 - 2\widehat{\mu}_n \mu]$ \\
$\text{Var}(Z) = n \cdot (\E[\widehat{\mu}_n^2] + \E[\mu^2] - \E[2\widehat{\mu}_n \mu])$ \\
$\text{Var}(Z) = n \cdot (\E[\widehat{\mu}_n^2] - \E[\widehat{\mu}_n]^2 + \E[\widehat{\mu}_n]^2 + \mu^2 - 2\mu\E[\widehat{\mu}_n])$ \\
$\text{Var}(Z) = n \cdot (\text{Var}(\widehat{\mu}_n) + \E[\widehat{\mu}_n]^2 + \mu^2 - 2\mu\E[\widehat{\mu}_n])$ \\
$\text{Var}(Z) = n \cdot (\frac{\sigma^2}{n} + \mu^2 + \mu^2 - 2\mu^2)$ \hfill Given \\
$\text{Var}(Z) = $ \framebox[1.1\width]{\textbf{$\sigma^2$}}

}

\section*{A.6}
{\Large 

If $f(x)$ is a PDF, the cumulative distribution function (CDF)
  is  defined as $F(x) = \int_{-\infty}^x f(y) dy$.  For any function
  $g : \R \rightarrow \R$ and random variable $X$ with PDF $f(x)$,
  recall that the expected value of $g(X)$ is defined as
  $\E[g(X)] = \int_{-\infty}^\infty g(y) f(y) dy$. For a boolean event
  $A$, define $\1\{ A \}$ as $1$ if $A$ is true, and $0$
  otherwise. Thus, $\1\{ x \leq a \}$ is $1$ whenever $x \leq a$ and
  $0$ whenever $x > a$.  Note that $F(x) = \E[\1\{X \leq x\}]$.  Let
  $X_1,\dots,X_n$ be \emph{independent and identically distributed}
  random variables with CDF $F(x)$. \\ 
  Define
  $\widehat{F}_n(x) = \frac{1}{n} \sum_{i=1}^n \1\{X_i \leq
  x\}$. Note, for every $x$,
  that $\widehat{F}_n(x)$ is an \emph{empirical estimate} of  $F(x)$.
  You may use your answers to the previous problem. \\

We are given that the empirical estimate of $F(x)$ as $\widehat{F}_n(x) = \frac{1}{n} \sum_{i=1}^n \1\{X_i \leq x\}$.

\subsection*{a.}
We aim to find $\E[ \widehat{F}_n(x) ]$ for any $x$: \\
$\E[ \widehat{F}_n(x) ] = \E[\frac{1}{n} \sum_{i=1}^n \1\{X_i \leq x\}] = \frac{1}{n} \sum_{i=1}^n \E[\1\{X_i \leq x\}] = \frac{1}{n} \sum_{i=1}^n \1\{X_i \leq x\} = $ \framebox[1.1\width]{\textbf{$F(x)$}}

\subsection*{b.}
For any $x$, the variance of $\widehat{F}_n(x)$ is $\E[ ( \widehat{F}_n(x) - F(x) )^2 ]$.  Show that $\textrm{Variance}(\widehat{F}_n(x)) = \frac{F(x)(1-F(x))}{n}$. \\

We aim to show that $\textrm{Var}(\widehat{F}_n(x)) = \frac{F(x)(1-F(x))}{n}$. We know that the variance of a Bernoulli random variable as we defined is $\text{Var}(X) = p(1-p)$, which tells us that $\text{Var}(F(x)) = F(x)(1-F(x))$ by definition in our case. Using our previous definitions, we know that \\
$\textrm{Var}(\widehat{F}_n(x)) = \frac{1}{n^2} \sum_{i = 1}^{n} Var(X_i)$ \\
$\textrm{Var}(\widehat{F}_n(x)) = \frac{1}{n^2} \cdot \sum_{i = 1}^{n} F(x)(1-F(x))$ \\ 
$\textrm{Var}(\widehat{F}_n(x)) = \frac{1}{n} \cdot F(x)(1-F(x))$ \\ 
$\textrm{Var}(\widehat{F}_n(x)) = \frac{F(x)(1-F(x))}{n}$ \\
as we sought to show.

% We know by definition that \\ 
% $\text{Var}(\widehat{F}_n(x)) = \E[ ( \widehat{F}_n(x) - F(x) )^2 ]$ \\
% $\text{Var}(\widehat{F}_n(x)) = \E[ \widehat{F}_n(x)^2 + F(x) ^2 - 2\widehat{F}_n(x)F(x)]$ \\
% $\text{Var}(\widehat{F}_n(x)) = \E[\widehat{F}_n(x)^2] + \E[F(x)^2] - 2\E[\widehat{F}_n(x)F(x)]$ \\
% $\text{Var}(\widehat{F}_n(x)) = \E[\widehat{F}_n(x)^2] + \E[F(x)^2] - 2\E[\widehat{F}_n(x)F(x)]$ \\

\subsection*{c.}
We aim to show that for all $x \in \R$, we have  $\displaystyle \E[ ( \widehat{F}_n(x) - F(x) )^2 ] \leq \tfrac{1}{4n}$. \\ 



}

\section*{A.7}
{\Large 

\subsection*{a.}
To find rank, we use row reduction and find how many unique non-zero rows are left once we reach row echelon form: \\ \\
$A = \begin{bmatrix} 1 & 2 & 1 \\ 1 & 0 & 3 \\ 1 & 1 & 2 \end{bmatrix}$ 
$ = \begin{bmatrix} 1 & 2 & 1 \\ 0 & -2 & 2 \\ 0 & -1 & 1 \end{bmatrix}$
$ = \begin{bmatrix} 1 & 2 & 1 \\ 0 & -2 & 2 \\ 0 & 0 & 0 \end{bmatrix}$ \\
With only 2 unique non-zero rows left, we can see that \framebox[1.1\width]{\textbf{the rank of $A$ is 2}} \\ \\
$B = \begin{bmatrix} 1 & 2 & 3 \\ 1 & 0 & 1 \\ 1 & 1 & 2 \end{bmatrix}$ 
$ = \begin{bmatrix} 1 & 2 & 3 \\ 0 & -2 & -2 \\ 0 & -1 & -1 \end{bmatrix}$
$ = \begin{bmatrix} 1 & 2 & 3 \\ 0 & -2 & -2 \\ 0 & 0 & 0 \end{bmatrix}$ \\ 
With only 2 unique non-zero rows left, we can see that \framebox[1.1\width]{\textbf{the rank of $B$ is 2}}

\subsection*{b.}
To find the minimal size basis for the column span of a matrix, we need to look at the pivot columns of the row echelon form matrix and then take those columns to get the basis:
$A = \begin{bmatrix} 1 & 2 & 1 \\ 1 & 0 & 3 \\ 1 & 1 & 2 \end{bmatrix}$ 
$ = \begin{bmatrix} 1 & 2 & 1 \\ 0 & -2 & 2 \\ 0 & -1 & 1 \end{bmatrix}$
$ = \begin{bmatrix} 1 & 2 & 1 \\ 0 & -2 & 2 \\ 0 & 0 & 0 \end{bmatrix}$ \\
Taking the first 2 columns, we get \\
$ = \begin{bmatrix} 1 & 2 \\ 1 & 0 \\ 1 & 1 \end{bmatrix}$ \\ \\
$B = \begin{bmatrix} 1 & 2 & 3 \\ 1 & 0 & 1 \\ 1 & 1 & 2 \end{bmatrix}$ 
$ = \begin{bmatrix} 1 & 2 & 3 \\ 0 & -2 & -2 \\ 0 & -1 & -1 \end{bmatrix}$
$ = \begin{bmatrix} 1 & 2 & 3 \\ 0 & -2 & -2 \\ 0 & 0 & 0 \end{bmatrix}$ \\ 
Taking the first 2 columns, we get \\
$ = \begin{bmatrix} 1 & 2 \\ 1 & 0 \\ 1 & 1 \end{bmatrix}$

}

\section*{A.8}
{\Large 

Let $A = \begin{bmatrix} 0 & 2 & 4 \\ 2 & 4 & 2 \\ 3 & 3 & 1 \end{bmatrix}$, $b = \begin{bmatrix} -2 & -2 & -4 \end{bmatrix}^T$, and $c=\begin{bmatrix} 1 & 1 & 1 \end{bmatrix}^T$.

\subsection*{a.}
$Ac = \begin{bmatrix} 0 & 2 & 4 \\ 2 & 4 & 2 \\ 3 & 3 & 1 \end{bmatrix} \cdot \begin{bmatrix} 1 & 1 & 1 \end{bmatrix}^T$ \\ 
$= \begin{bmatrix} 0 & 2 & 4 \\ 2 & 4 & 2 \\ 3 & 3 & 1 \end{bmatrix} \cdot \begin{bmatrix} 1 \\ 1 \\ 1 \\ \end{bmatrix}$ \\
$= \begin{bmatrix} 0 \cdot 1 & 2 \cdot 1 & 4 \cdot 1 \\ 2 \cdot 1 & 4 \cdot 1 & 2 \cdot 1 \\ 3 \cdot 1 & 3 \cdot 1 & 1 \cdot 1 \end{bmatrix}$ \\
$= \begin{bmatrix} 0 & 2 & 4 \\ 2 & 4 & 2 \\ 3 & 3 & 1 \end{bmatrix}$

\subsection*{b.}
$Ax = b$ \\
$\begin{bmatrix} 0 & 2 & 4 \\ 2 & 4 & 2 \\ 3 & 3 & 1 \end{bmatrix} x = \begin{bmatrix} -2 & -2 & -4 \end{bmatrix}^T$ \\ 
$\begin{bmatrix} 0 & 2 & 4 \\ 2 & 4 & 2 \\ 3 & 3 & 1 \end{bmatrix} x = \begin{bmatrix} -2 \\ -2 \\ -4 \end{bmatrix}$ \\ 
$\left[
\begin{array}{ccc|c}
0 & 2 & 4 & -2 \\
2 & 4 & 2 & -2 \\
3 & 3 & 1 & 4 \\
\end{array}
\right]$ =
$\left[
\begin{array}{ccc|c}
0 & 2 & 4 & -2 \\
2 & 4 & 2 & -2 \\
6 & 6 & 2 & 8 \\
\end{array}
\right]$ =
$\left[
\begin{array}{ccc|c}
0 & 2 & 4 & -2 \\
2 & 4 & 2 & -2 \\
0 & -6 & -4 & 14 \\
\end{array}
\right]$ =
$\left[
\begin{array}{ccc|c}
0 & 6 & 12 & -6 \\
2 & 4 & 2 & -2 \\
0 & -6 & -4 & 14 \\
\end{array}
\right]$ =
$\left[
\begin{array}{ccc|c}
0 & 0 & 8 & 8 \\
2 & 4 & 2 & -2 \\
0 & -6 & -4 & 14 \\
\end{array}
\right]$ =
$\left[
\begin{array}{ccc|c}
0 & 0 & 1 & 1 \\
1 & 2 & 1 & -1 \\
0 & -3 & -2 & 7 \\
\end{array}
\right]$ =
$\left[
\begin{array}{ccc|c}
0 & 0 & 1 & 1 \\
1 & 2 & 1 & -1 \\
0 & -3 & 0 & 9 \\
\end{array}
\right]$ =
$\left[
\begin{array}{ccc|c}
0 & 0 & 1 & 1 \\
1 & 2 & 1 & -1 \\
0 & 1 & 0 & -3 \\
\end{array}
\right]$ =
$\left[
\begin{array}{ccc|c}
0 & 0 & 1 & 1 \\
1 & 0 & 0 & 4 \\
0 & 1 & 0 & -3 \\
\end{array}
\right]$ \\ \\
So our result is therefore \\ \\
$\begin{bmatrix} 4 \\ -3 \\ 1 \end{bmatrix}$ 

}

\section*{A.9}
{\Large 

Assume $w$ is an $n$-dimensional vector and $b$ is a scalar. A hyperplane in $\R^n$ is the set $\{x : x\in \R^n,\text{ s.t. } w^T x + b = 0\}$.

\subsection*{a.}
($n=2$ example) Draw the hyperplane for $w=[-1,2]^T$, $b=2$? Label your axes.

\subsection*{b.}
($n=3$ example) Draw the hyperplane for $w=[1,1,1]^T$, $b=0$? Label your axes.

\subsection*{c.}
Given some $x_0 \in \R^n$, find the \emph{squared distance} to the hyperplane defined by $w^T x + b=0$.
	In other words, solve the following optimization problem:
	\begin{align*}
	\min_x& \|x_0 - x \|^2\\
	\text{s.t. }&w^Tx +b = 0
	\end{align*}
	(Hint: if $\widetilde{x}_0$ is the minimizer of the above problem, note that $\| x_0 - \widetilde{x}_0 \| = | \frac{w^T(x_0 - \widetilde{x}_0)}{\|w\|} |$. What is $w^T \widetilde{x}_0$?)

}

\section*{A.10}
{\Large 

For possibly non-symmetric $\mat{A}, \mat{B} \in \R^{n \times n}$ and $c \in \R$, let $f(x, y) = x^T \mat{A} x + y^T \mat{B} x + c$. Define $\nabla_z f(x,y) = \begin{bmatrix} \frac{\partial f(x,y)}{\partial z_1} & \frac{\partial f(x,y)}{\partial z_2} & \dots & \frac{\partial f(x,y)}{\partial z_n} \end{bmatrix}^T$.  

\subsection*{a.}
Explicitly write out the function $f(x, y)$ in terms of the components $A_{i,j}$ and $B_{i,j}$ using appropriate summations over the indices.

\subsection*{b.}
What is $\nabla_x f(x,y)$ in terms of the summations over indices \emph{and} vector notation?

\subsection*{c.}
What is $\nabla_y f(x,y)$ in terms of the summations over indices \emph{and} vector notation?

}

\section*{A.11}
{\Large 

\subsection*{a.}

% \begin{figure}[ht!]
%   \centering
%   \includegraphics[width=90mm]{pdf.PNG}
%   \caption{pdf \label{overflow}}
% \end{figure}

\subsection*{b.}


}

\section*{A.12}
{\Large 

Two random variables $X$ and $Y$ have equal
  distributions if their CDFs, $F_X$ and $F_Y$, respectively, are
  equal, i.e. for all $x$, $ |F_X(x) - F_Y(x)| = 0$. 
The central limit theorem says that the sum of $k$ independent,
zero-mean, variance-$1/k$ random variables converges to a (standard) Normal distribution as $k$ goes off to infinity.  
We will study this phenomenon empirically (you will use the Python packages Numpy and Matplotlib). 
Define $Y^{(k)} = \frac{1}{\sqrt{k}} \sum_{i=1}^k B_i$ where each $B_i$ is equal to $-1$ and $1$ with equal probability.
From your solution to problem 5, we know that $\frac{1}{\sqrt{k}} B_i$ is zero-mean and has variance $1/k$.

\subsection*{a.}
For $i=1,\dots,n$ let $Z_i \sim \mathcal{N}(0,1)$. If
  $F(x)$ is the true CDF from which each $Z_i$ is drawn (i.e.,
  Gaussian) and $\widehat{F}_n(x) = \frac{1}{n} \sum_{i=1}^n
  \1\{ Z_i \leq x)$, use the answer to problem 1.5  above to choose
  $n$ large enough such that, for all $x \in \R$, $ \sqrt{\E[
    (\widehat{F}_n(x)-F(x))^2 ]} \leq 0.0025$, and plot
  $\widehat{F}_n(x)$ from $-3$ to $3$. \\(Hint: use
  \texttt{Z=numpy.random.randn(n)} to generate the random
  variables, and \texttt{import matplotlib.pyplot as plt};\\
  \texttt{plt.step(sorted(Z), np.arange(1,n+1)/float(n))} to
  plot). 

\subsection*{b.}
For each $k \in \{1, 8, 64, 512\}$ generate $n$
  independent copies $Y^{(k)}$ and plot their empirical CDF on
  the same plot as part a.\\ (Hint: 
  $\texttt{np.sum(np.sign(np.random.randn(n,
    k))*np.sqrt(1./k), axis=1)}$ generates $n$ of the
  $Y^{(k)}$ random variables.) \\ \\ 

Be sure to always label your axes. 
Your plot should look something like the following (Tip: checkout \texttt{seaborn} for instantly better looking plots.)

}

\end{document}