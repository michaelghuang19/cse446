\documentclass{article}
\linespread{1.3}
\usepackage[margin=50pt]{geometry}
\usepackage{amsmath, amsthm, amssymb, amsthm, tikz, fancyhdr}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\newcommand{\changefont}{\fontsize{15}{15}\selectfont}

\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\E}{\mathbb{E}} 
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\field{R}} % real domain
% \newcommand{\C}{\field{C}} % complex domain
\newcommand{\F}{\field{F}} % functional domain

\newcommand{\T}{^{\textrm T}} % transpose

\def\diag{\text{diag}}

%% operator in linear algebra, functional analysis
\newcommand{\inner}[2]{#1\cdot #2}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\twonorm}[1]{\|#1\|_2^2}
% operator in functios, maps such as M: domain1 --> domain 2
\newcommand{\Map}[1]{\mathcal{#1}}
\renewcommand{\theenumi}{\alph{enumi}} 

\newcommand{\Perp}{\perp \! \! \! \perp}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\vct}[1]{\boldsymbol{#1}} % vector
\newcommand{\mat}[1]{\boldsymbol{#1}} % matrix
\newcommand{\cst}[1]{\mathsf{#1}} % constant
\newcommand{\ProbOpr}[1]{\mathbb{#1}}
\newcommand{\points}[1]{\small\textcolor{magenta}{\emph{[#1 points]}} \normalsize}
\date{{}}

\fancypagestyle{firstpageheader}
{
  \fancyhead[R]{\changefont Michael Huang \\ CSE 446 \\ Homework 3}
}

\begin{document}

\thispagestyle{firstpageheader}

\section*{A.1}
{\Large 

\framebox[1.1\width]{\textbf{answer}}

\subsection*{a.}

Say you trained an SVM classifier with an RBF kernel ($K(u, v) = \exp\left(-\frac{\norm{u-v}^2_2}{2\sigma^2}\right)$). It seems to underfit the training set: should you increase or decrease $\sigma$? \\

If the RBF kernel underfits the training set, we should decrease the bandwidth $\sigma$. Underfitting indicates a smoother fit and lower variance, so we want to decrease $\sigma$ so that we can decrease fit the training points a bit better.
% more narrow bumps and fits data more tightly

\subsection*{b.}

True or False: Training deep neural networks requires minimizing a non-convex loss function, and therefore gradient descent might not reach the globally-optimal solution. \\

True. With a non-convex loss function, gradient descent is not guaranteed to go all the way towards the global minimum.
% Could increase?

\subsection*{c.}

True or False: It is a good practice to initialize all weights to zero when training a deep neural network. \\

False. When training a deep neural network, we should initialize weights to random values, and all biases to 0.

\subsection*{d.}

True or False: We use non-linear activation functions in a neural networkâ€™s hidden layers so that the network learns non-linear decision boundaries. \\

True. We try to use non-linear activation functions in order to introduce non-linearity into the network. 
% If we only had linear activation functions, then the model would essentially be always one layer deep because of the way that linear functions naturally sum together.

\subsection*{e.}

True or False: Given a neural network, the time complexity of the backward pass step in the backpropagation algorithm can be prohibitively larger compared to the relatively low time complexity of the forward pass step. \\

False. The time complexity of the backward pass step and the forward pass step are the same. 
% The derivative of the forward pass can be calculated with the same complexity of the backward pass.

}

\section*{A.2}
{\Large

\framebox[1.1\width]{\textbf{answer}}

Suppose that our inputs $x$ are one-dimensional and that our feature map is infinite-dimensional: 
$\phi( x) $ is a vector whose $i$th component is \\
\[
    \frac{1}{\sqrt{i!}} e^{-x^2/2}x^i\ ,
\]
for all nonnegative integers $i$. (Thus, $\phi$ is an infinite-dimensional vector.)
Show that $K(x, x') = e^{-\frac{(x-x')^2}{2}}$ is a kernel function for this feature map, i.e., \\
\[
    \phi (x) \cdot \phi (x') = e^{-\frac{(x-x')^2}{2}}\ .
\]
Hint: Use the Taylor expansion of $z \mapsto e^z$.
(This is the one dimensional version of the Gaussian  (RBF) kernel).\\

}

\section*{A.3}
{\Large 

\framebox[1.1\width]{\textbf{answer}}

This problem will get you familiar with kernel ridge regression using the polynomial and RBF kernels.
First, let's generate some data. Let $n=30$ and $f_*(x) = 4 \sin(\pi x)\cos(6\pi x^2)$. For $i=1,\dots,n$ let each $x_i$ be drawn uniformly at random from $[0,1]$, and let $y_i = f_*(x_i) + \epsilon_i$ where $\epsilon_i \sim \mathcal{N}(0,1)$.

For any function $f$, the true error and the train error are respectively defined as
\begin{align*}
    \mathcal{E}_{\rm true}(f) = \E_{X,Y}[ ( f(X) - Y)^2], \quad \quad  \widehat{\mathcal{E}}_{\rm train}(f) =  \frac{1}{n} \sum_{i=1}^n \left(f(x_i)-y_i\right)^2.
\end{align*}
Using kernel ridge regression, construct a predictor
\[
  \widehat{\alpha} = \arg\min_\alpha \twonorm{K\alpha - y} + \lambda \alpha^\top K \alpha \ , \quad \quad \widehat{f}(x) = \sum_{i=1}^n \widehat{\alpha}_i k(x_i,x) 
\]
where $K\in\R^{n\times n}$ is the kernel matrix such that $K_{i,j} = k(x_i,x_j)$, and $\lambda\geq 0$ is the regularization constant. Include any code you use for your experiments in your submission.

\subsection*{a.}

Using leave-one-out cross validation, find a good $\lambda$ and hyperparameter settings for the following kernels:
  \begin{itemize}
    \item $k_{\rm poly}(x,z) = (1+x^\top z)^d$ where $d \in \mathbb{N}$ is a hyperparameter, 
    \item $k_{\rm rbf}(x,z) = \exp(-\gamma \twonorm{x-z})$ where $\gamma > 0$ is a hyperparameter. Given a dataset $x_1,\dots,x_n \in \R^d$, a heuristic for choosing a range of $\gamma$ in the right ballpark is the inverse of the median of all $\binom{n}{2}$ squared distances $\twonorm{x_i-x_j}$.
  \end{itemize} 
Report the values of $d$, $\gamma$, and the $\lambda$ values for both kernels.

\subsection*{b.}

Let $\widehat{f}_{\rm poly}(x)$ and $\widehat{f}_{\rm rbf}(x)$ be the functions learned using the hyperparameters you found in part a.
  For a single plot per function $\widehat{f} \in \left\lbrace \widehat{f}_{\rm poly}(x), \widehat{f}_{\rm rbf}(x) \right\rbrace$, plot the original data $\{(x_i,y_i)\}_{i=1}^n$, the true $f(x)$, and $\widehat{f}(x)$ (i.e., define a fine grid on $[0,1]$ to plot the functions).

\subsection*{c.}

We wish to build bootstrap percentile confidence intervals for $\widehat{f}_{\rm poly}(x)$ and $\widehat{f}_{\rm rbf}(x)$ for all $x \in [0,1]$ from part b. See Hastie, Tibshirani, Friedman Ch. 8.2 for a review of the bootstrap procedure.
  Use the non-parametric bootstrap with $B=300$ bootstrap iterations to find $5\%$ and $95\%$ percentiles at each point $x$ on a fine grid over $[0,1]$.
  
  \medskip
  
  Specifically, for each bootstrap sample $b \in \{1,\dots,B\}$, draw uniformly at randomly with replacement, $n$ samples from $\{(x_i,y_i)\}_{i=1}^n$, train an $\widehat{f}_b$ using the $b$-th resampled dataset, compute $\widehat{f}_b(x)$ for each $x$ in your fine grid; let the $5\%$ percentile at point $x$ be the largest value $\nu$ such that $\frac{1}{B} \sum_{b=1}^B \1\{ \widehat{f}_b(x) \leq \nu \} \leq .05$, define the $95\%$ percentile analogously. 
  
  \medskip
  
  Plot the $5$ and $95$ percentile curves on the plots from part b.

\subsection*{d.}

Repeat parts a, b, and c with $n=300$, but use $10$-fold CV instead of leave-one-out for part a.

\subsection*{e.}

For this problem, use the $\widehat{f}_{\rm poly}(x)$ and $\widehat{f}_{\rm rbf}(x)$ learned in part d. Suppose $m=1000$ additional samples $(x_1',y_1'),\dots,(x_m',y_m')$ are drawn i.i.d. the same way the first $n$ samples were drawn. 
  
  \medskip
  
  Use the non-parametric bootstrap with $B=300$ to construct a confidence interval on
  \[
    \E\left[ \left(Y-\widehat{f}_{\rm poly}(X)\right)^2 - \left(Y-\widehat{f}_{\rm rbf}(X)\right)^2 \right]
  \]
  (i.e. randomly draw with replacement $m$ samples denoted as $\{(\widetilde{x}_i',\widetilde{y}_i')\}_{i=1}^m$ from $\{(x_i',y_i')\}_{i=1}^m$ and compute $\frac{1}{m} \sum\limits_{i=1}^m \left[ \left(\widetilde{y}_i'-\widehat{f}_{\rm poly}(\widetilde{x}_i')\right)^2 - \left(\widetilde{y}_i'-\widehat{f}_{\rm rbf}(\widetilde{x}_i')\right)^2 \right]$, repeat this $B$ times) and find $5\%$ and $95\%$ percentiles. Report these values.
  
  \medskip
  
  Using this confidence interval, is there statistically significant evidence to suggest that one of $\widehat{f}_{\rm rbf}$ and $\widehat{f}_{\rm poly}$ is better than the other at predicting $Y$ from $X$? (Hint: does the confidence interval contain $0$?)

}

\section*{A.4}
{\Large 

\framebox[1.1\width]{\textbf{answer}}

\subsection*{a.}

Let $W_0 \in \mathbb{R}^{h \times d}$, $b_0 \in \mathbb{R}^h$, $W_1 \in \mathbb{R}^{k \times h}$, $b_1 \in \mathbb{R}^k$ and $\sigma(z)\colon \mathbb{R} \to \mathbb{R}$
some non-linear activation function applied element-wise. Given some $x \in \mathbb{R}^{d}$, the forward pass of the wide, shallow network can be formulated as:

\[
    \mathcal{F}_1(x) := W_1 \sigma(W_0 x + b_0) + b_1
\]


Use $h=64$ for the number of hidden units and choose an appropriate learning rate.
Train the network until it reaches $99\%$ accuracy on the training data and provide a training plot (loss vs. epoch).
Finally evaluate the model on the test data and report both the accuracy and the loss.

\subsection*{b.}

Let $W_0 \in \mathbb{R}^{h_0 \times d}$, $b_0 \in \mathbb{R}^{h_0}$, $W_1 \in \mathbb{R}^{h_1 \times h_0}$, $b_1 \in \mathbb{R}^{h_1}$,
$W_2 \in \mathbb{R}^{k \times h_1}$, $b_2 \in \mathbb{R}^{k}$ and $\sigma(z) : \mathbb{R} \rightarrow \mathbb{R}$
some non-linear activation function. Given some $x \in \mathbb{R}^{d}$, the forward pass of the network can be formulated as:

\[
    \mathcal{F}_2(x) := W_2 \sigma(W_1 \sigma(W_0 x + b_0) + b_1) + b_2
\]

Use $h_0 = h_1 = 32$ and perform the same steps as in part a.

\subsection*{c.}

Compute the total number of parameters of each network and report them. Then compare the number of parameters as well as the test
accuracies the networks achieved. Is one of the approaches (wide, shallow vs. narrow, deeper) better than the other? Give
an intuition for why or why not.

}

\section*{A.5}
{\Large 

\framebox[1.1\width]{\textbf{answer}}

So far we have trained very small neural networks from scratch. As mentioned in the previous problem, modern neural networks are much larger and more difficult to train and validate. In practice, it is rare to train such large networks from scratch. This is because it is difficult to obtain both the massive datasets and the computational resources required to train such networks. \\
 
 Instead of training a network from scratch, in this problem, we will use a network that has already been trained on a very large dataset (ImageNet) and adjust it for the task at hand. This process of adapting weights in a model trained for another task is known as \textit{transfer learning}.
\begin{itemize}
    \item Begin with the pretrained \text{AlexNet} model from \texttt{torchvision.models} for both tasks below. \text{AlexNet} achieved an early breakthrough performance on \texttt{ImageNet} and was instrumental in sparking the deep learning revolution in 2012.
    \item Do not modify any module within AlexNet that is not the final classifier layer.
    \item The output of AlexNet comes from the $6$-th layer of the classifier. Specifically, \texttt{model.classifer[6] = nn.Linear(4096, 1000)}. To use AlexNet with CIFAR-10, we will reinitialize (replace) this layer with \texttt{nn.Linear(4096, 10)}. This re-initializes the weights, and changes the output shape to reflect the desired number of target classes in CIFAR-10. 
\end{itemize}

\subsection*{a.}

\textbf{Use AlexNet as a fixed feature extractor:} Add a new linear layer to replace the existing classification layer, and only adjust the weights of this new layer (keeping the weights of all other layers fixed). Provide plots for training loss and validation loss over the number of epochs. Report the highest validation accuracy achieved. Finally, evaluate the model on the test data and report both the accuracy and the loss.  
    
    When using AlexNet as a fixed feature extractor, make sure to freeze all of the parameters in the network \textit{before} adding your new linear layer:
    \begin{verbatim}
    model = torchvision.models.alexnet(pretrained=True)
    for param in model.parameters():
        param.requires_grad = False
    model.classifier[6] = nn.Linear(4096, 10)
    \end{verbatim}

\subsection*{b.}

\textbf{Fine-Tuning:} The second approach to transfer learning is to fine-tune the weights of the pre-trained network, in addition to training the new classification layer. In this approach, all network weights are updated at every training iteration; we simply use the existing AlexNet weights as the ``initialization'' for our network (except for the weights in the new classification layer, which will be initialized using whichever method is specified in the constructor) prior to training on CIFAR-10. Following the same procedure, report all the same metrics and plots as in the previous question. 

}

\section*{A.6}
{\Large 

\framebox[1.1\width]{\textbf{answer}}

\subsection*{a.}

\textbf{Fully-connected output, 0 hidden layers (logistic regression):} this network has no hidden layers and linearly maps the input layer to the output layer. This can be written as 
  \begin{align*}
    x^{\rm out} &= W (x^{\rm in}) +b
  \end{align*} 
  
  where $x^{\rm out} \in \R^{10}$, $x^{\rm in} \in \R^{32 \times 32 \times 3}$, $W \in \R^{10 \times 3072}$, $b \in \R^{10}$ since $3072 = 32 \cdot 32 \cdot 3$. For a tensor $x \in \R^{a \times b \times c}$, we let $(x) \in \R^{a b c}$ be the reshaped form of the tensor into a vector (in an arbitrary but consistent pattern).

\subsection*{b.}

\textbf{Fully-connected output, 1 fully-connected hidden layer:} this network has one hidden layer denoted as $x^{\rm hidden} \in \R^{M}$ where $M$ will be a hyperparameter you choose ($M$ could be in the hundreds). The non-linearity applied to the hidden layer will be the \texttt{relu} ($\mathrm{relu}(x) = \max\{0,x\}$. This network can be written as
  \begin{align*}
    x^{\rm out} &= W_2 \mathrm{relu}(W_1 (x^{\rm in}) +b_1) + b_2
  \end{align*}
  where $W_1 \in \R^{M \times 3072}$, $b_1 \in \R^M$, $W_2 \in \R^{10 \times M}$, $b_2 \in \R^{10}$.

\subsection*{c.}

\textbf{Convolutional layer with max-pool and fully-connected output:} for a convolutional layer $W_1$ with filters of size $k \times k \times 3$, and $M$ filters (reasonable choices are $M=100$, $k=5$), we have that $\mathrm{Conv2d}(x^{\rm in}, W_1) \in \R^{(33-k) \times (33-k) \times M}$.
  
\begin{itemize}
    \item Each convolution will have its own offset applied to each of the output pixels of the convolution; we denote this as $\mathrm{Conv2d}(x^{\rm in}, W) + b_1$ where $b_1$ is parameterized in $\R^M$. Apply a \texttt{relu} activation to the result of the convolutional layer. 
    \item Next, use a max-pool of size $N \times N$ (a reasonable choice is $N=14$ to pool to $2 \times 2$ with $k=5$) we have that $\textrm{MaxPool}( \mathrm{relu}( \mathrm{Conv2d}(x^{\rm in}, W_1)+b_1)) \in \R^{\lfloor\frac{33-k}{N}\rfloor \times \lfloor\frac{33-k}{N}\rfloor \times M}$.
    \item We will then apply a fully-connected layer to the output to get a final network given as
        \begin{align*}
        x^{\rm output} = W_2 (\textrm{MaxPool}( \mathrm{relu}( \mathrm{Conv2d}(x^{\rm input}, W_1)+b_1))) + b_2
        \end{align*}
  where $W_2 \in \R^{10 \times M (\lfloor\frac{33-k}{N}\rfloor)^2}$, $b_2 \in \R^{10}$.
\end{itemize}

The parameters $M,k,N$ (in addition to the step size and momentum) are all hyperparameters, but you can choose a reasonable value. Tuning can be performed (optionally) in the next subproblem.

\subsection*{d.}

\textbf{Tuning:} Return to the original network you were left with at the end of the tutorial \emph{Training a classifier}. Tune the different hyperparameters (number of convolutional filters, filter sizes, dimensionality of the fully-connected layers, stepsize, etc.) and train for a sufficient number of iterations to achieve a \emph{test accuracy} of at least 70\%. Provide the hyperparameter configuration used to achieve this performance.

}

\end{document}