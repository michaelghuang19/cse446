\documentclass{article}
\linespread{1.3}
\usepackage[margin=50pt]{geometry}
\usepackage{amsmath, amsthm, amssymb, amsthm, tikz, fancyhdr}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\newcommand{\changefont}{\fontsize{15}{15}\selectfont}

\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\E}{\mathbb{E}} 
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\field{R}} % real domain
% \newcommand{\C}{\field{C}} % complex domain
\newcommand{\F}{\field{F}} % functional domain

\newcommand{\T}{^{\textrm T}} % transpose

\def\diag{\text{diag}}

%% operator in linear algebra, functional analysis
\newcommand{\inner}[2]{#1\cdot #2}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\twonorm}[1]{\|#1\|_2^2}
% operator in functios, maps such as M: domain1 --> domain 2
\newcommand{\Map}[1]{\mathcal{#1}}
\renewcommand{\theenumi}{\alph{enumi}} 

\newcommand{\Perp}{\perp \! \! \! \perp}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\vct}[1]{\boldsymbol{#1}} % vector
\newcommand{\mat}[1]{\boldsymbol{#1}} % matrix
\newcommand{\cst}[1]{\mathsf{#1}} % constant
\newcommand{\ProbOpr}[1]{\mathbb{#1}}
\newcommand{\points}[1]{\small\textcolor{magenta}{\emph{[#1 points]}} \normalsize}
\date{{}}

\fancypagestyle{firstpageheader}
{
  \fancyhead[R]{\changefont Michael Huang \\ CSE 446 \\ Homework 4}
}

\begin{document}

\thispagestyle{firstpageheader}

\section*{Collaborators}
{\Large 
Jimmy Guo, Neil Kagalwala, Andrew Wang
}
\section*{A.1}
{\Large 

\subsection*{a.}

True. If we are given a skinny matrix with $n$ samples $\gg d$ features, we know that the rank $k$ of the matrix can be no larger than its smallest dimension, which in our case will be $d$, so with full rank, we can possibly have zero construction error, although doing this would make using PCA considerably less uesful.
% This projection of our data using PCA will therefore be pretty useless, since no dimensionality reduction is performed, but it is doable and give us a our original representation of our data sans information loss. 

\subsection*{b.}

False. The columns of $V$ would be equal to the eigenvectors of $X^T X$, instead, rather than the rows.

\subsection*{c.}

False. We aim to minimize the intra-cluster distance between a given centroid and its clustered points, and for this objective, \textbf{ONLY} choosing a good k isn't necessarily a good  way to minimize this distance overall, although it could be a good supportive strategy. 

\subsection*{d.}

False. In general, the SVD of a matrix is not unique--if we have repeated singular values, then we can switch around the orders of values in $U$ and $V$.

\subsection*{e.}

False. In general, this is not true. The matrix must be symmetric in order for its rank to equal the number of nonzero eigenvalues.
% less than or equal to its rank

\subsection*{f.}

True. PCA uses pnly linear layers in contrast with the non-linear layers of the autoencoder, which are in turn more equipped to capture more features and more variance of the data in its representation.
% False. This is not necessarily the case. For example, with a limited number of dimensions, PCA will still try to seek out variance in its output, while the neural networks might tend to be more overfitted or have less developed layers that could lead to less acclimation to the variance in the data. There are considerations between the linear PCA map and the nonlinear activations with autoencoders as well.

}

\section*{A.2}
{\Large

\subsection*{a.}

\begin{enumerate}
  \item 
  % The weights in the solution $\widehat{w}_{\rm R}$ to the ridge regression problem ``shrinks'' as compared to the solution $\widehat{w}$ of the standard regression problem since the singular values in the solution to the ridge regression problem are smaller, which are in response to the smaller variance that is achieved. (as is the purpose of using ridge regression over standard regression).
  % Yes, using the singular value decomposition of X enables you to prove that the the ridge solution shrinks compared to the standard solution.

  We can use algebra, and express $\widehat{w}$ and $\widehat{w}_{\rm R}$ in terms of the SVD of $X$ and compare their L2-norms: \\ \\
  Standard Regression: \\
  $\min_w \twonorm{X w - y}$ \\
  $= \min_w (Xw - y)^\top(Xw-y)$ \\
  $\widehat{w} = (X^\top X)^{-1}X^\top y$ \\
  $= ((UDV^\top)^\top UDV^\top)^{-1}(UDV^\top)^\top y$ \\
  $= (VDU^\top UDV^\top)^{-1}VDU^\top y$ \\
  $= (VD^2V^\top)^{-1}VDU^\top y$ \\
  $= VD^{-2}V^\top VDU^\top y$ \\
  $= VD^{-1}U y$ \\

  Ridge Regression: \\
  $\min_w \twonorm{X w - y} + \lambda \twonorm{w}$ \\
  $= \min_w (Xw - y)^\top(Xw-y) + \lambda w^\top w$ \\
  $\widehat{w}_{\rm R} = (X^\top X + \lambda I)^{-1} X^\top y$ \\
  $= ((UDV^\top)^\top UDV^\top + \lambda I)^{-1} (UDV^\top)^\top y$ \\
  $= (VDU^\top UDV^\top + \lambda I)^{-1} VDU^\top y$ \\
  $= (VD^2V^\top + \lambda V V^\top)^{-1} VDU^\top y$ \\
  $= (V(D^2 + \lambda) V^\top)^{-1} VDU^\top y$ \\
  $= V(D^2 + \lambda)^{-1} V^\top VDU^\top y$ \\
  $= V(D^2 + \lambda)^{-1} DU^\top y$ \\

  We note that the key difference is that in $\widehat{w}$, our $D$-term is $D^{-1}$, while in $\widehat{w}_{\rm R}$, our $D$-term is $(D^2 + \lambda)^{-1}D$. We can compare this by transforming $D^{-1} = D^{-2}D$. The addition of a positive $\lambda$ term essentially shrinks this parameter and the overall $w$-solutions by increasing the factor by which we are dividing the $D$-term (and therefore overall term as well).

  \item 
  
  We first use SVD, and show that $UU^\top = I$: \\ 
  $UU^\top = X \Sigma V^\top (X \Sigma V^\top)^\top$ \\
  $= X \Sigma V^\top V \Sigma^\top X^\top$ \\
  $= X \Sigma \Sigma^\top X^\top$ \hfill Orthogonality of $V$, i.e. $V^\top V = I$ \\
  $= X X^\top$ \hfill Singular values in $\Sigma$ all 1, so $\Sigma \Sigma^\top = I I^\top = I$ \\
  $= I$ \hfill Orthogonality of $X$, i.e. $X X^\top = I$ \\ \\
  and that $U^\top U = I$ as well: \\
  $U^\top U = (X \Sigma V^\top)^\top X \Sigma V^\top$ \\
  $= V \Sigma^\top X^\top X \Sigma V^\top$ \\
  $= V \Sigma^\top \Sigma V^\top$ \hfill Orthogonality of $X$, i.e. $X^\top X = I$ \\
  $= V V^\top$ \hfill Singular values in $\Sigma$ all 1, so $\Sigma \Sigma^T = I I^T = I$ \\
  $= I$ \hfill Orthogonality of $V$, i.e. $V V^\top = I$ \\ \\

  We now aim to use this result to show that $U$ preserves Euclidean norms, i.e. $\norm{U x}_2 = \norm{x}_2$ for any $x\in \R^n$: \\
  $\norm{U x}^2_2 = (Ux)^\top Ux$ \hfill By the property that $\norm{x}^2_2 = x^\top x$ \\
  $= x^\top U^\top Ux$ \\
  $= x^\top x$ \hfill Since we showed $U^\top U = I$ \\
  $\norm{U x}^2_2 = \norm{x}^2_2$ \hfill By the aforementioned property \\
  $\norm{U x}_2 = \norm{x}_2$ \hfill Algebra

\end{enumerate}

}

\section*{A.3}
{\Large 

\subsection*{a.}

$\lambda_1 = 5.116787728342086$ \\
$\lambda_2 = 3.741328478864837$ \\
$\lambda_{10} = 1.2427293764173342$ \\
$\lambda_{30} = 0.364255720278894$ \\
$\lambda_{50} = 0.16970842700672767$ \\
$\sum_{i=1}^d{\lambda_i} = 52.72503549512691$

\subsection*{b.}

In terms of the first $k$ eigenvectors $V_k$, we can use what we have been doing so far, and adjusting for the mean value: \\
$V_k^\top V_k (x - \mu) + \mu$
% not sure if this perfectly captures 

\subsection*{c.}

\begin{figure}[h]
  \centering
  \includegraphics[width=130mm]{../hw4-code/results/a3_cobj.png}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=130mm]{../hw4-code/results/a3_cerr.png}
\end{figure}

\newpage

\subsection*{d.}

\begin{figure}[!hb]
  \centering
  \includegraphics[width=130mm]{../hw4-code/results/a3_d.png}
\end{figure}

The first 10 eigenvectors are essentially trying to capture the most core visual elements of a number, or the most distinguishing features that a person can generally use to pick out and easily categorize any given digit. Each component essentially captures the most variation in the data, which provides the most information for reconstructing the image.

\subsection*{e.}

\begin{figure}[!hb]
  \centering
  \includegraphics[width=130mm]{../hw4-code/results/a3_e.png}
\end{figure}

With more of the top eigenvectors, the image interpretation of each digit becomes much clearer. The digit becomes clearer with greater dimensionality, with the top 40 eigenvectors being good enough to tell each digit for sure, and just the first 15 eigenvectors being good enough to interpret for 6 and 7, and guessable for 2.

}

\section*{}
{\Large 

\newpage

\begin{verbatim}
# pca.py
# Applicable helpers for HW4 A3

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

import constants as c
import helpers as h

from mnist import MNIST
from scipy import linalg

def part_a(X, mu=None):
  n, d = X.shape

  if mu is None:
    mu = np.mean(X, axis=0)

  diff = X - mu
  sigma = np.matmul(diff.T, diff) / n

  lamb, v = np.linalg.eig(sigma)
  lamb = lamb.real
  v = v.real

  return lamb, v

def part_c(X_train, X_test):
  train_mse_data = []
  test_mse_data = []
  frac_data = []
  
  e_train, v_train = part_a(X_train)

  for i in range(c.k):
    recon_train = (v_train[:, :i+1]).dot(v_train[:, :i+1].T)
    recon_test = recon_train.dot(X_test.T).T
    recon_train = recon_train.dot(X_train.T).T

    mse_train = np.sum(np.square(recon_train - X_train)) / X_train.shape[0]
    mse_test = np.sum(np.square(recon_test - X_test)) / X_test.shape[0]
    
    train_mse_data.append(mse_train)
    test_mse_data.append(mse_test)

    frac = 1 - (np.sum(e_train[:i+1]) - np.sum(e_train))

    frac_data.append(frac)

  return train_mse_data, test_mse_data, frac_data

def part_d(v_list):
  fig, axes = plt.subplots(2, 5)

  axes_list = []
  for i, item in enumerate(axes):
    axes_list += list(item)

  for i, ax in enumerate(axes_list):
    img = v_list[:,i].reshape((28, 28))
    ax.imshow(img)

  fig.savefig(c.results_path + "a3_d")

def part_e(X_train, v_list):
  # for 2, 6, 7
  idx_set = [5, 13, 15]
  k_set = [5, 15, 40, 100]
  final_set = [X_train[idx_set[i], :] for i, _ in enumerate(idx_set)]

  for k in k_set:
    for idx in idx_set:
      image = (v_list[:, :k]).dot(v_list[:, :k].T)
      image = image.dot(X_train.T).T
      final_set.append(image[idx])

  fig, axes = plt.subplots(5, len(idx_set))

  for i, ax in enumerate(axes.ravel()):
    ax.imshow(final_set[i].reshape((28, 28)))

  fig.savefig(c.results_path + "a3_e")

def main():
  output = open(c.results_path + "a3.txt", "w")

  X_train, X_test = h.load_mnist()

  e_list, v_list = part_a(X_train)
  output.write(str([e_list[0], e_list[1], e_list[9],
                   e_list[29], e_list[49]]) + "\n")
  output.write(str(sum(e_list)) + "\n")

  train_mse_data, test_mse_data, frac_data = part_c(
      X_train, X_test)
  h.plot_multiple("error over k", "a3_cerr", "k", "error", 
                  [train_mse_data, test_mse_data], c.tt_list)
  h.plot_multiple("obj over k", "a3_cobj", "k", "obj", 
                  [frac_data], ["frac"])

  part_d(v_list) 

  part_e(X_train, v_list)

  output.close()


if __name__ == "__main__":
  main()

\end{verbatim}

\begin{verbatim}
# helpers.py
# Applicable helpers for HW3

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

import constants as c

from mnist import MNIST
from scipy import linalg

"""
Helper function for loading in MNIST data set
"""

def load_mnist():
  # load data
  mndata = MNIST(c.data_path)

  X_train, labels_train = map(np.array, mndata.load_training())
  X_test, labels_test = map(np.array, mndata.load_testing())
  X_train = X_train/255.0
  X_test = X_test/255.0

  return X_train, X_test

"""
Helper function for plotting multiple functions
"""
def plot_multiple(plt_title, img_title, x_label, y_label, data_list, legend_list):
  
  assert (len(data_list) > 0)

  iterations = list(range(1, len(data_list[0]) + 1))

  for data in data_list:
    plt.plot(iterations, data)

  plt.title(plt_title)
  plt.xlabel(x_label)
  plt.ylabel(y_label)
  plt.legend(legend_list)

  plt.savefig(c.results_path + img_title + c.png_exten)
  plt.close()

\end{verbatim}

\begin{verbatim}
# constants.py
# Applicable constants for HW4

import numpy as np
import os

home_dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

data_path = home_dir_path + '/data/'
results_path = home_dir_path + '/results/'

png_exten = '.png'

d = 784
k = 100

tt_list = ["training", "testing"]
\end{verbatim}

\newpage

}

\section*{A.4}
{\Large 
\subsection*{a.}

$h = 32$ error: 0.0014287357759972413 \\
$h = 64$ error: 0.0007750214679787557 \\
$h = 128$ error: 0.00036390629193435115 \\

\begin{figure}[!hb]
  \centering
  \includegraphics[width=110mm]{../hw4-code/results/a4_a32.png}
\end{figure}

\begin{figure}[!hb]
  \centering
  \includegraphics[width=110mm]{../hw4-code/results/a4_a64.png}
\end{figure}

\begin{figure}[!hb]
  \centering
  \includegraphics[width=110mm]{../hw4-code/results/a4_a128.png}
\end{figure}

\subsection*{b.}

$h = 32$ error: 0.0024316276252269746 \\
$h = 64$ error: 0.0020564391595621905 \\
$h = 128$ error: 0.0018853310883045196 \\

\begin{figure}[!hb]
  \centering
  \includegraphics[width=110mm]{../hw4-code/results/a4_b32.png}
\end{figure}

\begin{figure}[!hb]
  \centering
  \includegraphics[width=110mm]{../hw4-code/results/a4_b64.png}
\end{figure}

\begin{figure}[!hb]
  \centering
  \includegraphics[width=110mm]{../hw4-code/results/a4_b128.png}
\end{figure}

\subsection*{c.}

% Now, evaluate $\mathcal{F}_1(x)$ and $\mathcal{F}_2(x)$ (use $h = 128$     here) on the test set. Provide the test reconstruction errors in a table.

Test reconstruction error at $h = 128$ for $\mathcal{F}_1(x)$: 0.0003678079567849636 \\
Test reconstruction error at $h = 128$ for $\mathcal{F}_2(x)$: 0.001911275976896286

\subsection*{d.}

% In a few sentences, compare the quality of the reconstructions from these two autoencoders compare with those of PCA. You may want to re-run your code for PCA using the different $h$ values as the number of top-$k$ eigenvalues.

When comparing the quality of the reconstructions from the autoencoders to the PCA, with equal $h$ and $k$ values, there appeared to be a trend where the PCA would have much more recognizable digits quicker than the autencoders, but would generally improve less quickly than the autoencoders. However, the autoencoders often led to various transformations of the images (e.g. the outlines were sharper, darker, softer, etc.) that didn't lead to an exact transformation, while PCA seemed to essentially work towards the exact original image. Purely linear operations are also considerably faster than neural nets.

\newpage

\begin{verbatim}
# -*- coding: utf-8 -*-
"""cse446_hw4_a4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zOqDUJQ3NgzWO0o2wyGiaReabeSnYpyO
"""

# A4
# Imports

import copy
import os
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision.datasets as datasets
import torchvision.models as models
import torch.utils.data as data
from torchvision import transforms
from tqdm import tqdm
from torch.utils.data import DataLoader

# Constants 

device = "cuda" if torch.cuda.is_available() else "cpu"

# try different lrs
lr = 1E-3
batch_size = 128

h_set = [32, 64, 128]
idx_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

# import MNIST

# train_data = datasets.MNIST(root="./data", train=True, download=True, transform=transforms.ToTensor())
# test_data = datasets.MNIST(root="./data", train=False, download=True, transform=transforms.ToTensor())

train_data = datasets.MNIST(root="./data", train=True, download=True, transform=transforms.Compose([
                               transforms.ToTensor(),
                               transforms.Normalize(
                                 (0.1307,), (0.3081,))
                               ]))
test_data = datasets.MNIST(root="./data", train=False, download=True, transform=transforms.Compose([
                               transforms.ToTensor(),
                               transforms.Normalize(
                                 (0.1307,), (0.3081,))
                               ]))

train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)

# Helpers

def plot_single(data, title):
  print("plotting objective functions")

  iterations = list(range(1, len(data) + 1))

  plt.plot(iterations, )

  plt.title("objective function over time")
  plt.xlabel("iterations")
  plt.ylabel("objective function")

  plt.savefig(title)

def plot_loss(data, title):
  print("plotting loss")

  for item in data:
    plt.plot(k_set, item)

  plt.title("loss over time")
  plt.xlabel("iterations")
  plt.ylabel("loss")
  plt.legends(["test, training"])

  plt.savefig(title)

def train(model, learning_rate=lr, epochs=20):

  criterion = nn.MSELoss()
  optimizer = optim.Adam(model.parameters(), lr=lr)

  # we choose a good number for epochs
  for epoch in range(epochs):

    loss_sum = 0.0

    torch.set_grad_enabled(True)
    model.train()

    for inputs, _ in tqdm(train_loader):
      
      inputs = inputs.view(-1, 28 * 28)
      inputs = inputs.to(device)

      # zero the parameter gradients
      optimizer.zero_grad()

      # forward + backward + optimize
      outputs = model.forward(inputs)
      loss = criterion(inputs, outputs)
      loss.backward()
      optimizer.step()
        
      loss_sum += float(loss.item())
    
    loss_sum /= len(train_loader.dataset)

  return model, loss_sum

def evaluate(model):

  with torch.no_grad():
    for inputs, _ in train_loader:

      inputs = inputs.view(-1, 28 * 28)
      inputs = inputs.to(device)

      # get output by running image through the network 
      outputs = model.forward(inputs)

      return inputs.cpu().view(-1, 28, 28), outputs.cpu().view(-1, 28, 28)

def test(model):

  criterion = nn.MSELoss()

  loss_sum = 0.

  with torch.no_grad():
    for inputs, _ in tqdm(test_loader):

      inputs = inputs.view(-1, 28 * 28)
      inputs = inputs.to(device)

      # get output by running image through the network 
      outputs = model.forward(inputs)

      loss = criterion(inputs, outputs)
      loss_sum += float(loss.item())

  loss_sum /= len(test_loader.dataset)

  return loss_sum

"""# part (a)"""

a_model_list = []
loss_list = []

for h_val in h_set:
  a_model = nn.Sequential(
            nn.Linear(28 * 28, h_val),
            nn.Linear(h_val, 28 * 28)
          )

  a_model.to(device)

  model, loss = train(a_model)

  a_model_list.append(a_model)
  loss_list.append(loss)

print(loss_list)

# plot 32

train_samples, new_samples = evaluate(a_model_list[0])

fig, axes = plt.subplots(2, 10)
real_axes_list = []
new_axes_list = []
for i, item in enumerate(axes):
  if i == 0:
    real_axes_list += list(item)
  elif i == 1:
    new_axes_list += list(item)
  else:
    break

for i, ax in enumerate(real_axes_list):
  ax.imshow(train_samples[i])

for i, ax in enumerate(new_axes_list):
  ax.imshow(new_samples[i])

# plot 64

train_samples, new_samples = evaluate(a_model_list[1])

fig, axes = plt.subplots(2, 10)
real_axes_list = []
new_axes_list = []
for i, item in enumerate(axes):
  if i == 0:
    real_axes_list += list(item)
  elif i == 1:
    new_axes_list += list(item)
  else:
    break

for i, ax in enumerate(real_axes_list):
  ax.imshow(train_samples[i])

for i, ax in enumerate(new_axes_list):
  ax.imshow(new_samples[i])

# plot 128

train_samples, new_samples = evaluate(a_model_list[2])

fig, axes = plt.subplots(2, 10)
real_axes_list = []
new_axes_list = []
for i, item in enumerate(axes):
  if i == 0:
    real_axes_list += list(item)
  elif i == 1:
    new_axes_list += list(item)
  else:
    break

for i, ax in enumerate(real_axes_list):
  ax.imshow(train_samples[i])

for i, ax in enumerate(new_axes_list):
  ax.imshow(new_samples[i])

"""# part (b)"""

b_model_list = []
loss_list = []

for h_val in h_set:
  b_model = nn.Sequential(
            nn.Linear(28 * 28, h_val),
            nn.ReLU(),
            nn.Linear(h_val, 28 * 28),
            nn.ReLU()
          )
  
  b_model.to(device)

  model, loss = train(b_model)

  b_model_list.append(b_model)
  loss_list.append(loss)

print(loss_list)

# plot 32

train_samples, new_samples = evaluate(b_model_list[0])

fig, axes = plt.subplots(2, 10)
real_axes_list = []
new_axes_list = []
for i, item in enumerate(axes):
  if i == 0:
    real_axes_list += list(item)
  elif i == 1:
    new_axes_list += list(item)
  else:
    break

for i, ax in enumerate(real_axes_list):
  ax.imshow(train_samples[i])

for i, ax in enumerate(new_axes_list):
  ax.imshow(new_samples[i])

# plot 64

train_samples, new_samples = evaluate(b_model_list[1])

fig, axes = plt.subplots(2, 10)
real_axes_list = []
new_axes_list = []
for i, item in enumerate(axes):
  if i == 0:
    real_axes_list += list(item)
  elif i == 1:
    new_axes_list += list(item)
  else:
    break

for i, ax in enumerate(real_axes_list):
  ax.imshow(train_samples[i])

for i, ax in enumerate(new_axes_list):
  ax.imshow(new_samples[i])

# plot 128

train_samples, new_samples = evaluate(b_model_list[2])

fig, axes = plt.subplots(2, 10)
real_axes_list = []
new_axes_list = []
for i, item in enumerate(axes):
  if i == 0:
    real_axes_list += list(item)
  elif i == 1:
    new_axes_list += list(item)
  else:
    break

for i, ax in enumerate(real_axes_list):
  ax.imshow(train_samples[i])

for i, ax in enumerate(new_axes_list):
  ax.imshow(new_samples[i])

"""# part (c)"""

a_model = a_model_list[2]
b_model = b_model_list[2]

a_test_loss = test(a_model)
print(a_test_loss)

b_test_loss = test(b_model)
print(b_test_loss)

\end{verbatim}

\newpage

}

\section*{A.5}
{\Large 

\subsection*{a.}

\subsection*{b.}

\begin{figure}[!hb]
  \centering
  \includegraphics[width=110mm]{../hw4-code/results/a5_b.png}
\end{figure}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=110mm]{../hw4-code/results/a5_bcenter.png}
\end{figure}

\newpage

\subsection*{c.}

\begin{figure}[!hb]
  \centering
  \includegraphics[width=110mm]{../hw4-code/results/a5_c.png}
\end{figure}

\newpage

\begin{verbatim}
# -*- coding: utf-8 -*-
"""cse446_hw4_a5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SINxgwEQvCW2t4oVhfGCk7GH3LvjMkmZ
"""

# Commented out IPython magic to ensure Python compatibility.
# A5
# Imports

# %matplotlib inline

import os
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Constants 

k = 10
k_set = [2, 4, 8, 16, 32, 64]
# k_set = [10]

# Helpers

def plot_single(data, title):
  print("plotting objective functions")

  iterations = list(range(1, len(data) + 1))

  plt.plot(iterations, data)

  plt.title("objective function over time")
  plt.xlabel("iterations")
  plt.ylabel("objective function")

  plt.savefig(title)

def plot_loss(data, title):
  print("plotting loss")

  for item in data:
    plt.plot(k_set, item)

  plt.title("loss over time")
  plt.xlabel("iterations")
  plt.ylabel("loss")
  plt.legend(["test, training"])

  plt.savefig(title)

# import MNIST

train_data = datasets.MNIST(root="./data", train=True, download=True, transform=transforms.Compose([
                               transforms.ToTensor(),
                               transforms.Normalize(
                                 (0.1307,), (0.3081,))
                               ]))
test_data = datasets.MNIST(root="./data", train=False, download=True, transform=transforms.Compose([
                               transforms.ToTensor(),
                               transforms.Normalize(
                                 (0.1307,), (0.3081,))
                               ]))

train_loader = DataLoader(train_data, batch_size=len(train_data))
test_loader = DataLoader(test_data, batch_size=len(test_data))

train_data = next(iter(train_loader))[0].numpy()
test_data = next(iter(test_loader))[0].numpy()

def run_lloyd(data, k, function):
  n = data.shape[0]

  # initialize values
  old_idx = np.zeros(n)
  new_idx =  np.zeros(n)

  centroids = data[np.random.choice(n, size=k, replace=False)]

  result_list = []

  iteration = 0
  while True:
    print("iteration " + str(iteration))

    old_idx = np.copy(new_idx)
    new_idx = classify(data, centroids)

    centroids = recenter(data, new_idx, centroids)

    # calculating objective/error  

    if function == "b":
      error = np.square(np.min(np.linalg.norm(data - centroids[new_idx], axis = data.ndim - 1)))
      result_list.append(error)

    if function == "c":
      error = np.mean(np.square(np.min(np.linalg.norm(data - centroids[new_idx], axis = data.ndim - 1))))

    # if no more changes, then break
    if np.array_equal(old_idx, new_idx):
      break

    iteration +=1
  
  return centroids, result_list, error, iteration

def classify(data, centroids):
  print("classifying points to centroids")

  closest_centroids = []

  for point in data:
    closest_centroids.append(np.argmin([np.linalg.norm(point - centroid) for centroid in centroids]))

  return np.array(closest_centroids)

def recenter(data, closest, centroids):
  print("assigning centroids")
  
  new_centroids = []

  for i, centroid in enumerate(centroids):
    new_centroids.append(data[closest == i].mean(axis=0))

  return np.array(new_centroids)

def get_obj(data, centroids):
  obj = ((np.square(np.linalg.norm(data - centroids[new_idx], axis = data.ndim - 1))).sum() / n)

  return np.array(obj)

centroids, obj_list, _, iteration = run_lloyd(train_data, k, "b")

print(iteration)
print(obj_list)

# plot b
plot_single(obj_list, "a5.png")

# visualize centers

fig, axes = plt.subplots(2, 5)
axes_list = []
for item in axes:
  axes_list += list(item)

for i, ax in enumerate(axes_list):
  ax.imshow(centroids[i][0])

train_error_final = []
test_error_final = []

train_iter_counts = []
test_iter_counts = []

for k_val in k_set:
  _, _, last_train, train_iter = run_lloyd(train_data, k_val, "c")
  _, _, last_test, test_iter = run_lloyd(test_data, k_val, "c")

  train_error_final.append(last_train)
  test_error_final.append(last_test)

  train_iter_counts.append(train_iter)
  test_iter_counts.append(test_iter)

# plot c
print(train_iter_counts)
print(train_error_final)
print(test_iter_counts)
print(test_error_final)

plot_loss([train_error_final, test_error_final], "a5_b.png")
\end{verbatim}

\newpage

}

\section*{A.6}
{\Large 

In this question, you will explore how to apply machine learning theory and techniques to real-world problems. Each of the following statements details a setting, a dataset and a specific result we hope to achieve. Your job is to briefly (4-7 sentences) describe how you, if tasked with each of the below, would handle this scenario with the tools we’ve learned in this class. \\ \\
Your response should include any pre-processing steps you would take (i.e., data processing / acquisition), the specific machine learning pipeline you would use (i.e., algorithms and techniques learned in this class), and how your setup acknowledges the constraints and achieves the desired result. You should also aim to leverage some of the theory we have covered in this class. Some things to consider may be: the nature of the data (i.e., How hard is it to learn? Do we need more data? Are the data sources good?), the effectiveness of the pipeline (i.e., How strong is the model when properly trained and tuned?), and the time needed to effectively perform the pipeline. \\ \\
Because of the open-ended nature of the question, any thoroughly written responses will receive full credit.

\subsection*{a.}

The dataset is rich in multiple features, with labeled categories, but with something as serious as public health, I think that having a larger variety of data from multiple populations should be a priority so as to try to have more normalized data and weight data points respectively to reflect disparities such as underrepresented populations. I considered this problem in two different aspects: learning the factors that contribute to the disease, and predicting the disease susceptibility itself. With learning the factors, I would imagine that ridge regression might be useful in terms of easily and explicitly seeing which factors are more significant since certain coefficients will be set to zero, which would help emphasize feature selection itself. For the other half of the pipeline, I aimed to leverage deep learning's ability of inference and construct neural networks to maximize prediction accuracy. This comes at the cost of lack of factor interpretability, which is the reason I wanted to have a simpler ridge regression method as well. There are some notable downsides to using neural networks, however: need for a lot of data, long training time, high computational cost, etc. which might not make it worth it if we can't get a lot of data. 

\subsection*{b.}

To pre-process the data, I would first look into trying to make sure that I can relatively "standardize" images by focusing on a person's face and minimizing the background as much as possible. To do this, I would try to first use the Haar Cascade Classifier, the precursor to using HoG features as mentioned in lecture, and attempt to crop as closely onto the face as possible with all the samples. Although the data could have good variety since families usually contain a good variety of ages and genders, the dataset is still relatively small. Keeping this in mind, I didn't have as much confidence in using HoG feature extraction, I thought something like PCA would be more effective in finding meaningful features, and combining this with some sort of bootstrapping or cross-validation could help as well to verify that the model is working properly and assess if we need more data as well. I also chose a linear function since it is considerably faster than non-linear algorithms.

\subsection*{c.}

The dataset seems to be rich and have good variety, with data from around the world; this data is also generally not too difficult to acquire, such as through in-app automated or user reports that are pretty common for computer security programs. Since the metadata seems to be a mix of categorical and numerical data, I would first convert the categorical data to one-hot encoding so that it is easier to process as a real-valued vector. We could possibly use regularization for factors such as file sizes so as to avoid possible overfitting. With labeled data that will eventually be distributed between binary labels (malware or not), logistic regression makes sense here and is considerably faster and more lightweight than other models like neural networks. With a dataset that is relatively simple and well-structured, a scalable supervised learning solution like logistic regression fits well. 

}

\end{document}