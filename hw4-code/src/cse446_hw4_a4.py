# -*- coding: utf-8 -*-
"""cse446_hw4_a4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zOqDUJQ3NgzWO0o2wyGiaReabeSnYpyO
"""

# A4
# Imports

import copy
import os
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision.datasets as datasets
import torchvision.models as models
import torch.utils.data as data
from torchvision import transforms
from tqdm import tqdm
from torch.utils.data import DataLoader

# Constants 

device = "cuda" if torch.cuda.is_available() else "cpu"

# try different lrs
lr = 1E-3
batch_size = 128

h_set = [32, 64, 128]
idx_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

# import MNIST

# train_data = datasets.MNIST(root="./data", train=True, download=True, transform=transforms.ToTensor())
# test_data = datasets.MNIST(root="./data", train=False, download=True, transform=transforms.ToTensor())

train_data = datasets.MNIST(root="./data", train=True, download=True, transform=transforms.Compose([
                               transforms.ToTensor(),
                               transforms.Normalize(
                                 (0.1307,), (0.3081,))
                               ]))
test_data = datasets.MNIST(root="./data", train=False, download=True, transform=transforms.Compose([
                               transforms.ToTensor(),
                               transforms.Normalize(
                                 (0.1307,), (0.3081,))
                               ]))

train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)

# Helpers

def plot_single(data, title):
  print("plotting objective functions")

  iterations = list(range(1, len(data) + 1))

  plt.plot(iterations, )

  plt.title("objective function over time")
  plt.xlabel("iterations")
  plt.ylabel("objective function")

  plt.savefig(title)

def plot_loss(data, title):
  print("plotting loss")

  for item in data:
    plt.plot(k_set, item)

  plt.title("loss over time")
  plt.xlabel("iterations")
  plt.ylabel("loss")
  plt.legends(["test, training"])

  plt.savefig(title)

def train(model, learning_rate=lr, epochs=20):

  criterion = nn.MSELoss()
  optimizer = optim.Adam(model.parameters(), lr=lr)

  # we choose a good number for epochs
  for epoch in range(epochs):

    loss_sum = 0.0

    torch.set_grad_enabled(True)
    model.train()

    for inputs, _ in tqdm(train_loader):
      
      inputs = inputs.view(-1, 28 * 28)
      inputs = inputs.to(device)

      # zero the parameter gradients
      optimizer.zero_grad()

      # forward + backward + optimize
      outputs = model.forward(inputs)
      loss = criterion(inputs, outputs)
      loss.backward()
      optimizer.step()
        
      loss_sum += float(loss.item())
    
    loss_sum /= len(train_loader.dataset)

  return model, loss_sum

def evaluate(model):

  with torch.no_grad():
    for inputs, _ in train_loader:

      inputs = inputs.view(-1, 28 * 28)
      inputs = inputs.to(device)

      # get output by running image through the network 
      outputs = model.forward(inputs)

      return inputs.cpu().view(-1, 28, 28), outputs.cpu().view(-1, 28, 28)

def test(model):

  criterion = nn.MSELoss()

  loss_sum = 0.

  with torch.no_grad():
    for inputs, _ in tqdm(test_loader):

      inputs = inputs.view(-1, 28 * 28)
      inputs = inputs.to(device)

      # get output by running image through the network 
      outputs = model.forward(inputs)

      loss = criterion(inputs, outputs)
      loss_sum += float(loss.item())

  loss_sum /= len(test_loader.dataset)

  return loss_sum

"""# part (a)"""

a_model_list = []
loss_list = []

for h_val in h_set:
  a_model = nn.Sequential(
            nn.Linear(28 * 28, h_val),
            nn.Linear(h_val, 28 * 28)
          )

  a_model.to(device)

  model, loss = train(a_model)

  a_model_list.append(a_model)
  loss_list.append(loss)

print(loss_list)

# plot 32

train_samples, new_samples = evaluate(a_model_list[0])

fig, axes = plt.subplots(2, 10)
real_axes_list = []
new_axes_list = []
for i, item in enumerate(axes):
  if i == 0:
    real_axes_list += list(item)
  elif i == 1:
    new_axes_list += list(item)
  else:
    break

for i, ax in enumerate(real_axes_list):
  ax.imshow(train_samples[i])

for i, ax in enumerate(new_axes_list):
  ax.imshow(new_samples[i])

# plot 64

train_samples, new_samples = evaluate(a_model_list[1])

fig, axes = plt.subplots(2, 10)
real_axes_list = []
new_axes_list = []
for i, item in enumerate(axes):
  if i == 0:
    real_axes_list += list(item)
  elif i == 1:
    new_axes_list += list(item)
  else:
    break

for i, ax in enumerate(real_axes_list):
  ax.imshow(train_samples[i])

for i, ax in enumerate(new_axes_list):
  ax.imshow(new_samples[i])

# plot 128

train_samples, new_samples = evaluate(a_model_list[2])

fig, axes = plt.subplots(2, 10)
real_axes_list = []
new_axes_list = []
for i, item in enumerate(axes):
  if i == 0:
    real_axes_list += list(item)
  elif i == 1:
    new_axes_list += list(item)
  else:
    break

for i, ax in enumerate(real_axes_list):
  ax.imshow(train_samples[i])

for i, ax in enumerate(new_axes_list):
  ax.imshow(new_samples[i])

"""# part (b)"""

b_model_list = []
loss_list = []

for h_val in h_set:
  b_model = nn.Sequential(
            nn.Linear(28 * 28, h_val),
            nn.ReLU(),
            nn.Linear(h_val, 28 * 28),
            nn.ReLU()
          )
  
  b_model.to(device)

  model, loss = train(b_model)

  b_model_list.append(b_model)
  loss_list.append(loss)

print(loss_list)

# plot 32

train_samples, new_samples = evaluate(b_model_list[0])

fig, axes = plt.subplots(2, 10)
real_axes_list = []
new_axes_list = []
for i, item in enumerate(axes):
  if i == 0:
    real_axes_list += list(item)
  elif i == 1:
    new_axes_list += list(item)
  else:
    break

for i, ax in enumerate(real_axes_list):
  ax.imshow(train_samples[i])

for i, ax in enumerate(new_axes_list):
  ax.imshow(new_samples[i])

# plot 64

train_samples, new_samples = evaluate(b_model_list[1])

fig, axes = plt.subplots(2, 10)
real_axes_list = []
new_axes_list = []
for i, item in enumerate(axes):
  if i == 0:
    real_axes_list += list(item)
  elif i == 1:
    new_axes_list += list(item)
  else:
    break

for i, ax in enumerate(real_axes_list):
  ax.imshow(train_samples[i])

for i, ax in enumerate(new_axes_list):
  ax.imshow(new_samples[i])

# plot 128

train_samples, new_samples = evaluate(b_model_list[2])

fig, axes = plt.subplots(2, 10)
real_axes_list = []
new_axes_list = []
for i, item in enumerate(axes):
  if i == 0:
    real_axes_list += list(item)
  elif i == 1:
    new_axes_list += list(item)
  else:
    break

for i, ax in enumerate(real_axes_list):
  ax.imshow(train_samples[i])

for i, ax in enumerate(new_axes_list):
  ax.imshow(new_samples[i])

"""# part (c)"""

a_model = a_model_list[2]
b_model = b_model_list[2]

a_test_loss = test(a_model)
print(a_test_loss)

b_test_loss = test(b_model)
print(b_test_loss)