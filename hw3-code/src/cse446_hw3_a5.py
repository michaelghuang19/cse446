# -*- coding: utf-8 -*-
"""cse446_hw3_a5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11aNA4mlWdyL4rPJEsDzZmBsNRF1as1nm
"""

# A5
# Imports

import copy
import os
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision.datasets as datasets
import torchvision.models as models
import torch.utils.data as data
from torchvision import transforms
from tqdm import tqdm

# Constants
device = "cuda" if torch.cuda.is_available() else "cpu"

learning_rate = 1E-3
epochs = 20
val_pct = 0.1
batch_size = 128

labels = ['train', 'val']

data_transforms = {
      'train': transforms.Compose([
          transforms.RandomResizedCrop(224),
          transforms.RandomHorizontalFlip(),
          transforms.ToTensor(),
          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
      ]),
      'val': transforms.Compose([
          transforms.Resize(256),
          transforms.CenterCrop(224),
          transforms.ToTensor(),
          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
      ]),
      # yeah this is the same for now
      'test': transforms.Compose([
          transforms.Resize(256),
          transforms.CenterCrop(224),                                
          transforms.ToTensor(),
          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
      ]),
  }

# Helpers

def get_dataset():

  train_data = datasets.CIFAR10(root="./data/train", train=True, download=True, transform=data_transforms['train'])
  val_data = datasets.CIFAR10(root="./data/val", train=True, download=True, transform=data_transforms['val'])
  test_data = datasets.CIFAR10(root="./data/test", train=False, download=True, transform=data_transforms['test'])

  # break up into training/validation
  train_idx = np.random.permutation(np.arange(len(train_data)))
  val_data = data.Subset(val_data, train_idx[0:int(val_pct * len(train_data))])
  train_data = data.Subset(train_data, train_idx[int(val_pct * len(train_data)):])

  # dataloaders
  train_loader = data.DataLoader(train_data, batch_size=batch_size, shuffle=True)
  val_loader = data.DataLoader(val_data, batch_size=batch_size, shuffle=True)
  test_loader = data.DataLoader(test_data, batch_size=batch_size, shuffle=True)

  return train_loader, val_loader, test_loader

train_loader, val_loader, test_loader = get_dataset()

def plot_train_val_loss(train_data, val_data, title):
  print("plotting loss")

  epochs = list(range(1, len(train_data) + 1))

  print(train_data)
  print(val_data)

  plt.plot(epochs, train_data)
  plt.plot(epochs, val_data)

  plt.title("loss over time")
  plt.xlabel("epochs")
  plt.ylabel("loss")
  
  plt.legend(labels)

  plt.savefig(title)

def train_model(model, criterion, optimizer, num_epochs=20):
  train_loss_list = []
  val_loss_list = []
  val_acc_list = []

  best_model_wts = copy.deepcopy(model.state_dict())
  best_acc = 0.0

  for epoch in range(num_epochs):

    train_loss = 0.0
    model.train()

    for inputs, labels in tqdm(train_loader):
      inputs, labels = inputs.to(device), labels.to(device)
      optimizer.zero_grad()
      
      with torch.set_grad_enabled(True):
        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

      train_loss += float(loss.item())
    
    train_loss /= len(train_loader.dataset)
    print(train_loss)
    train_loss_list.append(train_loss)

    val_loss = 0.0
    val_acc = 0.0
    model.eval()
        
    for inputs, labels in tqdm(val_loader):
      inputs, labels = inputs.to(device), labels.to(device)
      optimizer.zero_grad()

      with torch.set_grad_enabled(False):
        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)
        loss = criterion(outputs, labels)

      val_loss += float(loss.item())
      val_acc += torch.sum(preds == labels.data)
        
    val_loss /= len(val_loader.dataset)
    val_acc = val_acc.double() / len(val_loader.dataset)
    
    val_loss_list.append(val_loss)
    val_acc_list.append(val_acc)

    if val_acc > best_acc:
      best_acc = val_acc
      best_model_wts = copy.deepcopy(model.state_dict())
  
  model.load_state_dict(best_model_wts)
  return model, train_loss_list, val_loss_list, val_acc_list

def test_alex(model):
  acc = 0.
  loss_sum = 0.
  total = 0.

  model.eval()

  with torch.no_grad():
    for inputs, labels in tqdm(test_loader):
      inputs, labels = inputs.to(device), labels.to(device)

      outputs = model(inputs)
      _, preds = torch.max(outputs.data, 1)
      loss = criterion(outputs, labels)

      loss_sum += float(loss.item())
      acc += (preds == labels).sum().item()
      total += labels.size(0)
        
  loss_sum /= total
  acc /= total

  return acc, loss_sum

######################## part (a) ########################

fixed_model = models.alexnet(pretrained=True)
for param in fixed_model.parameters():
  param.requires_grad = False
fixed_model.classifier[6] = nn.Linear(4096, 10)
fixed_model = fixed_model.to(device)

criterion = nn.CrossEntropyLoss(reduction='sum')
optimizer = torch.optim.Adam(fixed_model.parameters(), learning_rate)

fixed_model, a_train_loss, a_val_loss, a_val_acc = train_model(fixed_model, criterion, optimizer, epochs)
a_test_acc, a_test_loss = test_alex(fixed_model)

plot_train_val_loss(a_train_loss, a_val_loss, "a5_a.png")
print(max(a_val_acc))
print(a_test_acc)
print(a_test_loss)

######################## part (b) ########################

fine_model = models.alexnet(pretrained=True)
fine_model.classifier[6] = nn.Linear(4096, 10)
fine_model = fine_model.to(device)

criterion = nn.CrossEntropyLoss(reduction='sum')
optimizer = torch.optim.Adam(fine_model.parameters(), learning_rate)

fine_model, b_train_loss, b_val_loss, b_val_acc = train_model(fine_model, criterion, optimizer, epochs)
b_test_acc, b_test_loss = test_alex(fine_model)

plot_train_val_loss(b_train_loss, b_val_loss, "a5_b.png")
print(max(b_val_acc))
print(b_test_acc)
print(b_test_loss)