# -*- coding: utf-8 -*-
"""cse446_hw3_a4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_SIbwG3BXMDcx1ZjGFNXcLkcuS4C7G_O
"""

# A4
# Imports

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision.datasets as datasets
from torchvision import transforms
from tqdm import tqdm

# Constants

learning_rate = 1E-3

# import MNIST

train_data = datasets.MNIST(root="./data", train=True, download=True, transform=transforms.ToTensor())
test_data = datasets.MNIST(root="./data", train=False, download=True, transform=transforms.ToTensor())

train_loader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=True)

# Helpers

def initialize_weights(mode, h=64, h_0 = 32, h_1 = 32, m=784, n=10):
  print("Initializing weights for " + mode)
  
  alpha = 1 / np.sqrt(m)
  
  d = m
  k = n
  
  if mode == "a":
    W_0 = nn.init.uniform_(torch.empty(h, d), -alpha, alpha)
    b_0 = nn.init.uniform_(torch.empty(h), -alpha, alpha)
    
    W_1 = nn.init.uniform_(torch.empty(k, h), -alpha, alpha)
    b_1 = nn.init.uniform_(torch.empty(k), -alpha, alpha)
    
    W_0.requires_grad = True
    b_0.requires_grad = True
    W_1.requires_grad = True
    b_1.requires_grad = True
    
    return [W_0, W_1, b_0, b_1]

  if mode == "b":
    W_0 = nn.init.uniform_(torch.empty(h_0, d), -alpha, alpha)    
    b_0 = nn.init.uniform_(torch.empty(h_0), -alpha, alpha)
    
    W_1 = nn.init.uniform_(torch.empty(h_1, h_0), -alpha, alpha)
    b_1 = nn.init.uniform_(torch.empty(h_1), -alpha, alpha)

    W_2 = nn.init.uniform_(torch.empty(k, h_1), -alpha, alpha)
    b_2 = nn.init.uniform_(torch.empty(k), -alpha, alpha)

    W_0.requires_grad = True
    b_0.requires_grad = True
    W_1.requires_grad = True
    b_1.requires_grad = True
    W_2.requires_grad = True
    b_2.requires_grad = True

    return [W_0, W_1, W_2, b_0, b_1, b_2]

def f_1(x, W_0, W_1, b_0, b_1):
  temp = torch.matmul(x, W_0.T) + b_0
  return torch.matmul(F.relu(temp), W_1.T) + b_1

def f_2(x, W_0, W_1, W_2, b_0, b_1, b_2):
  temp1 = F.relu(torch.matmul(x, W_0.T) + b_0)
  temp2 = F.relu(torch.matmul(temp1, W_1.T) + b_1)
  return torch.matmul(temp2, W_2.T) + b_2 

def plot_loss(data, title):
  print("plotting loss")

  epochs = list(range(1, len(data) + 1))

  plt.plot(epochs, data)

  plt.title("loss over time")
  plt.xlabel("epochs")
  plt.ylabel("loss")

  plt.savefig(title)

def train_a(params, learning_rate, h=64, m=784, n=10):
  print("training a")

  W_0, W_1, b_0, b_1 = params[0], params[1], params[2], params[3]  

  optimizer = optim.Adam(params, lr=learning_rate)
  
  loss_data = []
  
  while True:

    acc = 0.
    loss_sum = 0.

    for x, y in tqdm(train_loader):
      x = x.view(-1, m)

      predictions = f_1(x, W_0, W_1, b_0, b_1)

      acc += torch.sum(torch.argmax(predictions, dim=1) == y)

      loss = F.cross_entropy(predictions, y)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

      loss_sum += loss
    
    loss_data.append(loss_sum / len(train_loader.dataset))
    acc = acc / len(train_loader.dataset)

    if acc >= 0.99:
      break
  
  return loss_data, [W_0, W_1, b_0, b_1]

def test_a(params):
  print("testing a")

  acc = 0.
  loss_sum = 0.

  W_0, W_1, b_0, b_1 = params[0], params[1], params[2], params[3]

  for x, y in tqdm(test_loader):
    x = torch.flatten(x, start_dim=1, end_dim=3)

    predictions = f_1(x, W_0, W_1, b_0, b_1)

    acc += torch.sum(torch.argmax(predictions, dim=1) == y)
    loss_sum += F.cross_entropy(predictions, y)
  
  acc /= len(test_loader.dataset)
  loss_sum /= len(test_loader.dataset)

  return acc, loss_sum

######################## part (a) ########################

params = initialize_weights("a")
a_train_loss, params = train_a(params, learning_rate)
plot_loss(a_train_loss, "a4_a")

a_test_acc, a_test_loss = test_a(params)
print(a_test_acc)
print(a_test_loss)

def train_b(params, learning_rate, h=32, m=784):
  print("training b")

  W_0, W_1, W_2, b_0, b_1, b_2 = params[0], params[1], params[2], params[3], params[4], params[5]

  optimizer = optim.Adam(params, lr=learning_rate)
    
  acc_data = []
  loss_data = []

  while True:

    acc = 0.
    loss_sum = 0.

    for x, y in tqdm(train_loader):
      x = x.view(-1, m)

      predictions = f_2(x, W_0, W_1, W_2, b_0, b_1, b_2)

      acc += torch.sum(torch.argmax(predictions, dim=1) == y)

      loss = F.cross_entropy(predictions, y)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

      loss_sum += loss
    
    loss_data.append(loss_sum / len(train_loader.dataset))
    acc = acc / len(train_loader.dataset)
    
    if acc >= 0.99:
      break
  
  return loss_data, [W_0, W_1, W_2, b_0, b_1, b_2]

def test_b(params):
  print("testing b")

  acc = 0.
  loss_sum = 0.
  
  W_0, W_1, W_2, b_0, b_1, b_2 = params[0], params[1], params[2], params[3], params[4], params[5]
  
  for x, y in tqdm(test_loader):
    x = torch.flatten(x, start_dim=1, end_dim=3)

    predictions = f_2(x, W_0, W_1, W_2, b_0, b_1, b_2)

    acc += torch.sum(torch.argmax(predictions, dim=1) == y)
    loss_sum += F.cross_entropy(predictions, y)
  
  acc /= len(test_loader.dataset)
  loss_sum /= len(test_loader.dataset)

  return acc, loss_sum

######################## part (b) ########################
params = initialize_weights("b")
b_train_loss, params = train_b(params, learning_rate)
plot_loss(b_train_loss, "a4_b")

b_test_acc, b_test_loss = test_b(params)
print(b_test_acc)
print(b_test_loss)
